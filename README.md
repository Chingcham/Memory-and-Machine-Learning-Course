### Lecturer

Hoa T. Le

Contact me at <first_name>.<last_name>@loria.fr 
or at my offfice B213 (Loria) (please make an appointment first).

# Overview

The aim of this course is to introduce computational, numerical and distributed memories from a theoretical and epistemological standpoint as well as neural networks and their use in cognitive science. Concerning machine learning, the course will focus on various model learners such as Markov Chains, Hidden Markov Model, Reinforcement Learning and Neural Networks.

# Target audience

This course is for Master 1 Science Cognitive and Applications (Master Erasmus Mundus – University of Lorraine). This is an introduction course, assuming no prior knowledge of Machine Learning.

# Course Organization

- 30 hours = 10 work sessions of 3 hours/week
- Courses = half lectures / half exercises or practicals
- Evaluation: group projects (2 to 3 students/group)
- The last 2 (maybe 3) work sessions will be saved to work on the project

# Syllabus

## Lecture 1. Introduction about Artificial Intelligence

Readings
* [Deep learning. Yann LeCun,	Yoshua Bengio	& Geoffrey Hinton. Nature 2015.](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true)
* [Human-level control through Deep Reinforcement Learning. Mnih et al., Nature 2015.](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* [Mastering the game of Go with deep neural networks and tree search. Silver et al., Nature 2016.](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)
* [Hybrid computing using a neural network with dynamic external memory. Graves et al., Nature 2016.](https://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz)
* [Neuroscience-Inspired Artificial Intelligence. Hassabis et al., Neuron 2017.](https://deepmind.com/documents/113/Neuron.pdf)

More Readings
* [The Rise of Computer-Aided Explanation. Nielsen. QuantaMagazine 2015.](https://www.quantamagazine.org/the-rise-of-computer-aided-explanation-20150723)
* [Will Computers Redefine the Roots of Math ? Hartnett. QuantaMagazine 2015.](https://www.quantamagazine.org/univalent-foundations-redefines-mathematics-20150519)
* [Mapping the Brain to Build Better Machines. Singer, QuantaMagazine 2016.](https://www.quantamagazine.org/mapping-the-brain-to-build-better-machines-20160406)

Practical: Learning basic PyTorch [open tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)
* What is PyTorch ?
* Initialization and matrix computation
* Conversion between PyTorch <-> Numpy
* Autograd: automatic differentiation package

## Lecture 2. Optimization

- Deterministic methods
- Stochastic methods

Readings
* [Chapter 1, 2 & 4 of Optimization Book.](https://www.amazon.com/Iterative-Methods-Optimization-Frontiers-Mathematics/dp/0898714338)


## Lecture 3. Neural Networks

- Convolution Neural Networks
- Recurrent Neural Networks
- Memory Networks

Readings
* [Chapter 9 & 10 of Deep Learning Book.](http://www.deeplearningbook.org/contents/TOC.html)

More Readings
* [Learning representations by back-propagating errors. DE Rumelhart, GE Hinton, RJ Williams. Nature 1986.](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf)
* [Intriguing properties of neural networks. Szegedy et al., ICLR 2014.](https://arxiv.org/abs/1312.6199)
* [A Neural Algorithm of Artistic Style. Gatys et al., Arxiv 2015.](https://arxiv.org/abs/1508.06576)
* [End-To-End Memory Networks. Sukhbaatar et al., NIPS 2015.](https://arxiv.org/abs/1503.08895)
* [Google's Neural Machine Translation. Arxiv 2016.](https://arxiv.org/abs/1609.08144)
* [The loss surface of deep and wide neural networks. Nguyen and Hein. ICML 2017.](https://arxiv.org/abs/1704.08045)
* [Do Convolutional Networks need to be Deep for Text Classification ? Le et al., Arxiv 2017.](https://arxiv.org/abs/1707.04108)
* [Ask the Right Questions: Active Question Reformulation with Reinforcement Learning. Buck et al., Arxiv 2017.](https://arxiv.org/abs/1705.07830)

## Lecture 4. Reinforcement Learning

- Markov Decision Processes
- Dynamic Programming
- Monte-Carlo learning
- TD-learning

Readings
* [Chapter 4, 5 & 6 of Reinforcement Learning Book.](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981)

More Readings 
* [AlphaGo by DeepMind.](https://deepmind.com/research/alphago/)
* [Is AlphaGo Really Such a Big Deal ? Nielsen. QuantaMagazine 2016.](https://www.quantamagazine.org/is-alphago-really-such-a-big-deal-20160329/)
* [AlphaGo, in context. Karpathy. Medium Blog 2017.](https://medium.com/@karpathy/alphago-in-context-c47718cb95a5)


## Lecture 5. Sequential Statistical Modeling

- Bayesian learning
- Markov chains
- Hidden Markov Model

Readings
* [Chapter 1 & 2 of HMM Book.](https://www.amazon.com/Inference-Hidden-Markov-Springer-Statistics/dp/0387402640)


## Book References
- Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto (1998). 
- Iterative Methods for Optimization. C. T. Kelley (1999). 
- The Elements of Statistical Learning. H. Friedman, Robert Tibshirani and Trevor Hastie (2001). 
- Inference in Hidden Markov Models. Olivier Cappé, Eric Moulines and Tobias Rydén (2005). 
- Pattern Recognition and Machine Learning. Christopher M. Bishop (2006). 
- Deep Learning. Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016). 

## 20% projects

* [Link](https://github.com/lethienhoa/Memory-and-Machine-Learning-Course/blob/master/twenty_percent_project.md)
